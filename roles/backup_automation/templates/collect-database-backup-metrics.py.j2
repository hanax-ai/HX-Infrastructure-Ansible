#!/usr/bin/env python3
"""
Database Backup Metrics Collection Script
Generated by Ansible - DO NOT EDIT MANUALLY
"""

import os
import sys
import time
import json
import logging
import ssl
from pathlib import Path
from datetime import datetime, timedelta
from http.server import HTTPServer, BaseHTTPRequestHandler
from collections import deque

# Configuration
BACKUP_BASE = Path("{{ backup_automation.base_directory }}/database")
LOG_FILE = Path("{{ backup_automation.log_directory }}/backup-metrics.log")
METRICS_PORT = {{ backup_automation.monitoring.metrics_port | default(9090) }}
BIND_ADDRESS = "{{ backup_automation.monitoring.bind_address | default('0.0.0.0') }}"
RETENTION_HOURS = {{ backup_automation.monitoring.retention_hours | default(168) }}

# TLS Configuration
TLS_ENABLED = {{ backup_automation.monitoring.tls_enabled | default(true) | lower }}
TLS_CERT_FILE = "{{ backup_automation.monitoring.tls_cert_file | default('/etc/ssl/certs/backup-metrics.crt') }}"
TLS_KEY_FILE = "{{ backup_automation.monitoring.tls_key_file | default('/etc/ssl/private/backup-metrics.key') }}"

# Logging configuration
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler(LOG_FILE),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)
BACKUP_BASE = Path("{{ backup_automation.base_directory }}/database")
LOG_FILE = Path("{{ backup_automation.log_directory }}/backup-metrics.log")
METRICS_PORT = {{ backup_automation.monitoring.metrics_port }}
BIND_ADDRESS = "{{ backup_automation.monitoring.bind_address | default('127.0.0.1') }}"
METRICS_FILE = Path("/var/lib/backup/metrics/backup-status.txt")

# Setup logging
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s [%(levelname)s] %(message)s',
    handlers=[
        logging.FileHandler(LOG_FILE),
        logging.StreamHandler()
    ]
)
logger = logging.getLogger(__name__)

class BackupMetrics:
    def __init__(self):
        self.metrics = {}
        
    def collect_backup_metrics(self):
        """Collect backup-related metrics"""
        logger.info("Collecting backup metrics")
        
        # Initialize metrics
        self.metrics = {
            'backup_count_total': 0,
            'backup_size_bytes_total': 0,
            'backup_age_seconds_oldest': 0,
            'backup_age_seconds_newest': 0,
            'backup_disk_usage_percent': 0,
            'backup_last_success_timestamp': 0,
            'backup_last_failure_timestamp': 0
        }
        
        # Collect PostgreSQL metrics
        {% if database_backup.databases.postgresql.enabled %}
        self._collect_service_metrics("postgresql")
        {% endif %}
        
        # Collect Redis metrics
        {% if database_backup.databases.redis.enabled %}
        self._collect_service_metrics("redis")
        {% endif %}
        
        # Collect disk usage
        self._collect_disk_metrics()
        
        # Collect backup status from logs
        self._collect_status_metrics()
        
    def _collect_service_metrics(self, service):
        """Collect metrics for a specific service"""
        service_dir = BACKUP_BASE / service
        if not service_dir.exists():
            logger.warning(f"Service directory does not exist: {service_dir}")
            return
            
        backup_files = []
        for pattern in ["*.sql*", "*.rdb*", "*.dump*", "*.gz", "*.enc"]:
            backup_files.extend(service_dir.glob(pattern))
            
        if not backup_files:
            logger.warning(f"No backup files found for {service}")
            return
            
        # Count and size
        service_count = len(backup_files)
        service_size = sum(f.stat().st_size for f in backup_files if f.is_file())
        
        self.metrics['backup_count_total'] += service_count
        self.metrics['backup_size_bytes_total'] += service_size
        
        # Age metrics
        if backup_files:
            ages = [time.time() - f.stat().st_mtime for f in backup_files if f.is_file()]
            if ages:
                oldest_age = max(ages)
                newest_age = min(ages)
                
                if self.metrics['backup_age_seconds_oldest'] == 0 or oldest_age > self.metrics['backup_age_seconds_oldest']:
                    self.metrics['backup_age_seconds_oldest'] = oldest_age
                    
                if self.metrics['backup_age_seconds_newest'] == 0 or newest_age < self.metrics['backup_age_seconds_newest']:
                    self.metrics['backup_age_seconds_newest'] = newest_age
        
        logger.info(f"Collected metrics for {service}: {service_count} files, {service_size} bytes")
        
    def _collect_disk_metrics(self):
        """Collect disk usage metrics"""
        try:
            import shutil
            total, used, free = shutil.disk_usage(BACKUP_BASE.parent)
            usage_percent = (used / total) * 100
            self.metrics['backup_disk_usage_percent'] = usage_percent
            logger.info(f"Disk usage: {usage_percent:.1f}%")
        except Exception as e:
            logger.error(f"Failed to collect disk metrics: {e}")
            
    def _collect_status_metrics(self):
        """Collect backup status from log files"""
        log_dir = Path("{{ backup_automation.log_directory }}")
        if not log_dir.exists():
            return
            
        # Look for recent success/failure timestamps
        for log_file in log_dir.glob("*.log"):
            try:
                with open(log_file, 'r') as f:
                    lines = list(deque(f, maxlen=200))  # Tail last 200 lines efficiently
                    
                for line in lines:
                    if "completed successfully" in line.lower():
                        # Extract timestamp
                        try:
                            timestamp_str = line.split()[0] + " " + line.split()[1]
                            timestamp = datetime.strptime(timestamp_str, "%Y-%m-%d %H:%M:%S")
                            timestamp_unix = timestamp.timestamp()
                            if timestamp_unix > self.metrics['backup_last_success_timestamp']:
                                self.metrics['backup_last_success_timestamp'] = timestamp_unix
                        except:
                            pass
                            
                    elif "failed" in line.lower() or "error" in line.lower():
                        try:
                            timestamp_str = line.split()[0] + " " + line.split()[1]
                            timestamp = datetime.strptime(timestamp_str, "%Y-%m-%d %H:%M:%S")
                            timestamp_unix = timestamp.timestamp()
                            if timestamp_unix > self.metrics['backup_last_failure_timestamp']:
                                self.metrics['backup_last_failure_timestamp'] = timestamp_unix
                        except:
                            pass
            except Exception as e:
                logger.error(f"Failed to parse log file {log_file}: {e}")
                
    def generate_prometheus_metrics(self):
        """Generate Prometheus-format metrics"""
        metrics_text = []
        
        # Add help and type information
        metrics_text.extend([
            "# HELP backup_count_total Total number of backup files",
            "# TYPE backup_count_total counter",
            f"backup_count_total {self.metrics['backup_count_total']}",
            "",
            "# HELP backup_size_bytes_total Total size of all backup files in bytes",
            "# TYPE backup_size_bytes_total counter", 
            f"backup_size_bytes_total {self.metrics['backup_size_bytes_total']}",
            "",
            "# HELP backup_age_seconds_oldest Age of oldest backup file in seconds",
            "# TYPE backup_age_seconds_oldest gauge",
            f"backup_age_seconds_oldest {self.metrics['backup_age_seconds_oldest']}",
            "",
            "# HELP backup_age_seconds_newest Age of newest backup file in seconds", 
            "# TYPE backup_age_seconds_newest gauge",
            f"backup_age_seconds_newest {self.metrics['backup_age_seconds_newest']}",
            "",
            "# HELP backup_disk_usage_percent Backup storage disk usage percentage",
            "# TYPE backup_disk_usage_percent gauge",
            f"backup_disk_usage_percent {self.metrics['backup_disk_usage_percent']:.2f}",
            "",
            "# HELP backup_last_success_timestamp Unix timestamp of last successful backup",
            "# TYPE backup_last_success_timestamp gauge",
            f"backup_last_success_timestamp {self.metrics['backup_last_success_timestamp']}",
            "",
            "# HELP backup_last_failure_timestamp Unix timestamp of last backup failure",
            "# TYPE backup_last_failure_timestamp gauge", 
            f"backup_last_failure_timestamp {self.metrics['backup_last_failure_timestamp']}",
            "",
            "# HELP backup_metrics_collection_timestamp Unix timestamp of metrics collection",
            "# TYPE backup_metrics_collection_timestamp gauge",
            f"backup_metrics_collection_timestamp {time.time()}",
        ])
        
        return "\n".join(metrics_text)

class MetricsHandler(BaseHTTPRequestHandler):
    def __init__(self, backup_metrics, *args, **kwargs):
        self.backup_metrics = backup_metrics
        super().__init__(*args, **kwargs)
        
    def do_GET(self):
        if self.path == "/metrics":
            # Collect fresh metrics
            self.backup_metrics.collect_backup_metrics()
            metrics_text = self.backup_metrics.generate_prometheus_metrics()
            
            self.send_response(200)
            self.send_header('Content-Type', 'text/plain; charset=utf-8')
            self.end_headers()
            self.wfile.write(metrics_text.encode('utf-8'))
            try:
                METRICS_FILE.write_text(metrics_text)
            except Exception:
                logger.warning("Failed to write metrics file", exc_info=True)
            
        elif self.path == "{{ backup_automation.monitoring.health_check_endpoint }}":
            # Health check endpoint
            self.send_response(200)
            self.send_header('Content-Type', 'application/json')
            self.end_headers()
            health_data = {
                "status": "healthy",
                "timestamp": time.time(),
                "service": "backup-metrics"
            }
            self.wfile.write(json.dumps(health_data).encode('utf-8'))
            
        else:
            self.send_response(404)
            self.end_headers()
            
    def log_message(self, format, *args):
        # Suppress default HTTP logging
        pass

def main():
    # Create necessary directories
    LOG_FILE.parent.mkdir(parents=True, exist_ok=True)
    METRICS_FILE.parent.mkdir(parents=True, exist_ok=True)
    
    logger.info("Starting backup metrics collection service")
    
    # Initialize metrics collector
    backup_metrics = BackupMetrics()
    
    # Create HTTP server
    def handler(*args, **kwargs):
        return MetricsHandler(backup_metrics, *args, **kwargs)
        
    # Create HTTPS server with TLS support
    if TLS_ENABLED and os.path.exists(TLS_CERT_FILE) and os.path.exists(TLS_KEY_FILE):
        server = HTTPServer((BIND_ADDRESS, METRICS_PORT), handler)
        context = ssl.SSLContext(ssl.PROTOCOL_TLS_SERVER)
        context.load_cert_chain(TLS_CERT_FILE, TLS_KEY_FILE)
        server.socket = context.wrap_socket(server.socket, server_side=True)
        protocol = "https"
    else:
        server = HTTPServer((BIND_ADDRESS, METRICS_PORT), handler)
        protocol = "http"
        logger.warning("TLS not enabled or certificates not found, using HTTP")
    
    logger.info(f"Metrics server listening on port {METRICS_PORT}")
    logger.info(f"Metrics endpoint: {protocol}://localhost:{METRICS_PORT}/metrics")
    logger.info(f"Health check endpoint: {protocol}://localhost:{METRICS_PORT}{{ backup_automation.monitoring.health_check_endpoint }}")
    
    try:
        server.serve_forever()
    except KeyboardInterrupt:
        logger.info("Shutting down metrics server")
        server.shutdown()

if __name__ == "__main__":
    main()
