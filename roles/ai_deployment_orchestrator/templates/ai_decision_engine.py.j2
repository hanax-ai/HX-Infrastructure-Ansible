#!/usr/bin/env python3
"""
AI-Driven Deployment Decision Engine
Intelligent deployment strategy selection based on system metrics and ML models
"""

import json
import logging
import pickle
import numpy as np
import pandas as pd
from datetime import datetime, timedelta
from sklearn.ensemble import RandomForestClassifier
from sklearn.preprocessing import StandardScaler
import requests
import os

class DeploymentDecisionEngine:
    def __init__(self, config_path='/opt/hx/ai-orchestrator/config.yml'):
        self.config = self._load_config(config_path)
        self.model = None
        self.scaler = StandardScaler()
        self.logger = self._setup_logging()
        
    def _load_config(self, config_path):
        """Load configuration from YAML file"""
        import yaml
        with open(config_path, 'r') as f:
            return yaml.safe_load(f)
    
    def _setup_logging(self):
        """Setup logging configuration"""
        logging.basicConfig(
            level=logging.INFO,
            format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',
            handlers=[
                logging.FileHandler('/var/log/hx-orchestrator/decision_engine.log'),
                logging.StreamHandler()
            ]
        )
        return logging.getLogger(__name__)
    
    def load_model(self):
        """Load trained ML model"""
        model_path = self.config['ai_orchestrator']['decision_engine']['model_path']
        try:
            with open(model_path, 'rb') as f:
                self.model = pickle.load(f)
            self.logger.info(f"Model loaded from {model_path}")
        except FileNotFoundError:
            self.logger.warning("No trained model found, using rule-based fallback")
            self.model = None
    
    def collect_metrics(self):
        """Collect current system metrics for decision making"""
        metrics = {}
        
        # Prometheus metrics
        try:
            prom_url = "{{ prometheus_url | default('http://localhost:9090') }}"
            
            # CPU utilization
            cpu_query = 'avg(100 - (avg by (instance) (rate(node_cpu_seconds_total{mode="idle"}[5m])) * 100))'
            cpu_response = requests.get(f"{prom_url}/api/v1/query", params={'query': cpu_query})
            metrics['cpu_utilization'] = float(cpu_response.json()['data']['result'][0]['value'][1])
            
            # Memory utilization
            mem_query = '(1 - (node_memory_MemAvailable_bytes / node_memory_MemTotal_bytes)) * 100'
            mem_response = requests.get(f"{prom_url}/api/v1/query", params={'query': mem_query})
            metrics['memory_utilization'] = float(mem_response.json()['data']['result'][0]['value'][1])
            
            # Error rate
            error_query = 'rate(http_requests_total{status=~"5.."}[5m]) / rate(http_requests_total[5m]) * 100'
            error_response = requests.get(f"{prom_url}/api/v1/query", params={'query': error_query})
            if error_response.json()['data']['result']:
                metrics['error_rate'] = float(error_response.json()['data']['result'][0]['value'][1])
            else:
                metrics['error_rate'] = 0.0
            
            # Traffic load
            traffic_query = 'rate(http_requests_total[5m])'
            traffic_response = requests.get(f"{prom_url}/api/v1/query", params={'query': traffic_query})
            if traffic_response.json()['data']['result']:
                metrics['traffic_load'] = float(traffic_response.json()['data']['result'][0]['value'][1])
            else:
                metrics['traffic_load'] = 0.0
                
        except Exception as e:
            self.logger.error(f"Failed to collect Prometheus metrics: {e}")
            # Fallback values
            metrics.update({
                'cpu_utilization': 50.0,
                'memory_utilization': 60.0,
                'error_rate': 0.1,
                'traffic_load': 100.0
            })
        
        # Additional context
        metrics.update({
            'hour_of_day': datetime.now().hour,
            'day_of_week': datetime.now().weekday(),
            'is_maintenance_window': self._is_maintenance_window(),
            'recent_deployments': self._count_recent_deployments()
        })
        
        return metrics
    
    def _is_maintenance_window(self):
        """Check if current time is within maintenance window"""
        now = datetime.now()
        # Default maintenance window: 2-4 AM
        return 2 <= now.hour <= 4
    
    def _count_recent_deployments(self):
        """Count deployments in the last 24 hours"""
        try:
            log_file = '/var/log/hx-orchestrator/deployments.log'
            if not os.path.exists(log_file):
                return 0
            
            count = 0
            cutoff = datetime.now() - timedelta(hours=24)
            
            with open(log_file, 'r') as f:
                for line in f:
                    try:
                        log_data = json.loads(line)
                        log_time = datetime.fromisoformat(log_data['timestamp'])
                        if log_time > cutoff:
                            count += 1
                    except:
                        continue
            
            return count
        except:
            return 0
    
    def calculate_risk_score(self, metrics):
        """Calculate deployment risk score based on current metrics"""
        risk_factors = self.config['ai_orchestrator']['risk_assessment']['factors']
        weights = {
            'traffic_patterns': 0.3,
            'system_health': 0.4,
            'historical_performance': 0.2,
            'resource_utilization': 0.1
        }
        
        risk_score = 0.0
        
        # Traffic patterns risk
        if metrics['traffic_load'] > 1000:
            risk_score += weights['traffic_patterns'] * 0.8
        elif metrics['traffic_load'] > 500:
            risk_score += weights['traffic_patterns'] * 0.5
        else:
            risk_score += weights['traffic_patterns'] * 0.2
        
        # System health risk
        if metrics['error_rate'] > 1.0:
            risk_score += weights['system_health'] * 0.9
        elif metrics['error_rate'] > 0.5:
            risk_score += weights['system_health'] * 0.6
        else:
            risk_score += weights['system_health'] * 0.1
        
        # Resource utilization risk
        cpu_risk = min(metrics['cpu_utilization'] / 100, 1.0)
        mem_risk = min(metrics['memory_utilization'] / 100, 1.0)
        resource_risk = max(cpu_risk, mem_risk)
        risk_score += weights['resource_utilization'] * resource_risk
        
        # Historical performance (simplified)
        if metrics['recent_deployments'] > 5:
            risk_score += weights['historical_performance'] * 0.7
        elif metrics['recent_deployments'] > 2:
            risk_score += weights['historical_performance'] * 0.4
        else:
            risk_score += weights['historical_performance'] * 0.1
        
        return min(risk_score, 1.0)
    
    def select_deployment_strategy(self, metrics, environment='prod'):
        """Select optimal deployment strategy using AI/ML or rules"""
        
        if self.model is not None:
            # Use ML model for decision
            features = self._extract_features(metrics)
            prediction = self.model.predict_proba([features])[0]
            confidence = max(prediction)
            
            if confidence >= self.config['ai_orchestrator']['decision_engine']['confidence_threshold']:
                strategy_idx = np.argmax(prediction)
                strategies = ['blue_green', 'canary', 'rolling']
                selected_strategy = strategies[strategy_idx]
                self.logger.info(f"ML model selected: {selected_strategy} (confidence: {confidence:.3f})")
                return selected_strategy, confidence
        
        # Fallback to rule-based decision
        risk_score = self.calculate_risk_score(metrics)
        thresholds = self.config['ai_orchestrator']['risk_assessment']['thresholds']
        
        if environment in ['dev', 'test']:
            # Less conservative for non-prod
            if risk_score < thresholds['medium_risk']:
                return 'rolling', 0.8
            else:
                return 'blue_green', 0.7
        
        # Production environment
        if risk_score < thresholds['low_risk']:
            if metrics['is_maintenance_window']:
                return 'rolling', 0.9
            else:
                return 'blue_green', 0.8
        elif risk_score < thresholds['medium_risk']:
            return 'canary', 0.7
        else:
            return 'blue_green', 0.6
    
    def _extract_features(self, metrics):
        """Extract features for ML model"""
        return [
            metrics['cpu_utilization'],
            metrics['memory_utilization'],
            metrics['error_rate'],
            metrics['traffic_load'],
            metrics['hour_of_day'],
            metrics['day_of_week'],
            int(metrics['is_maintenance_window']),
            metrics['recent_deployments']
        ]
    
    def make_deployment_decision(self, environment='prod'):
        """Main method to make deployment decision"""
        try:
            # Collect current metrics
            metrics = self.collect_metrics()
            
            # Calculate risk score
            risk_score = self.calculate_risk_score(metrics)
            
            # Select strategy
            strategy, confidence = self.select_deployment_strategy(metrics, environment)
            
            # Log decision
            decision = {
                'timestamp': datetime.now().isoformat(),
                'environment': environment,
                'strategy': strategy,
                'confidence': confidence,
                'risk_score': risk_score,
                'metrics': metrics
            }
            
            self.logger.info(f"Deployment decision: {strategy} (risk: {risk_score:.3f}, confidence: {confidence:.3f})")
            
            # Save decision log
            with open('/var/log/hx-orchestrator/decisions.log', 'a') as f:
                f.write(json.dumps(decision) + '\n')
            
            return decision
            
        except Exception as e:
            self.logger.error(f"Error making deployment decision: {e}")
            # Emergency fallback
            return {
                'timestamp': datetime.now().isoformat(),
                'environment': environment,
                'strategy': 'blue_green',
                'confidence': 0.5,
                'risk_score': 0.8,
                'error': str(e)
            }

if __name__ == '__main__':
    engine = DeploymentDecisionEngine()
    engine.load_model()
    decision = engine.make_deployment_decision()
    print(json.dumps(decision, indent=2))
